{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project goal is to create an ETL pipeline using the I94 immigration dataset and the city temperature data from Kaggle to enable users to analyse the relationship between immgration behaviours and the temperature of a location.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, col, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "ETL pipeline using the I94 immigration dataset and the city temperature data from Kaggle.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 immigration dataset\n",
    "\n",
    "```\n",
    "I94YR   - 4 digit year\n",
    "I94MON  - Numeric month\n",
    "I94CIT  - Country visited\n",
    "I94PORT - Port visited\n",
    "I94MODE - Mode of travel\n",
    "I94ADDR - Address\n",
    "I94BIR  - Age of Respondent in Years\n",
    "I94VISA - Purpose of visit. Visa codes collapsed into three categories.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_header_file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3096313, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_df = pd.read_csv(temperature_fname)\n",
    "temperature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2016.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94yr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94mon.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 692.,  254.,  101.,  102.,  103.,  104.,  105.,  107.,  108.,\n",
       "        109.,  110.,  111.,  112.,  113.,  114.,  115.,  116.,  117.,\n",
       "        118.,  119.,  121.,  122.,  123.,  124.,  126.,  127.,  128.,\n",
       "        129.,  130.,  131.,  133.,  135.,  140.,  141.,  145.,  147.,\n",
       "        148.,  151.,  152.,  153.,  154.,  155.,  156.,  157.,  159.,\n",
       "        162.,  163.,  164.,  165.,  166.,  167.,  201.,  203.,  204.,\n",
       "        206.,  207.,  209.,  213.,  218.,  220.,  236.,  242.,  244.,\n",
       "        245.,  249.,  250.,  251.,  253.,  255.,  256.,  257.,  258.,\n",
       "        260.,  261.,  262.,  263.,  266.,  267.,  268.,  272.,  273.,\n",
       "        274.,  296.,  297.,  298.,  299.,  301.,  304.,  310.,  311.,\n",
       "        315.,  316.,  320.,  323.,  324.,  326.,  329.,  332.,  336.,\n",
       "        338.,  339.,  340.,  342.,  343.,  344.,  345.,  348.,  350.,\n",
       "        352.,  368.,  369.,  370.,  371.,  372.,  373.,  375.,  376.,\n",
       "        382.,  383.,  384.,  385.,  386.,  387.,  388.,  389.,  390.,\n",
       "        391.,  392.,  397.,  399.,  407.,  413.,  417.,  438.,  441.,\n",
       "        464.,  465.,  472.,  504.,  509.,  512.,  513.,  514.,  516.,\n",
       "        518.,  519.,  520.,  522.,  523.,  524.,  525.,  526.,  527.,\n",
       "        528.,  574.,  575.,  576.,  577.,  579.,  581.,  582.,  584.,\n",
       "        585.,  586.,  602.,  603.,  687.,  688.,  689.,  690.,  691.,\n",
       "        693.,  694.,  695.,  696.,  714.,  718.,  727.,  732.,  734.,\n",
       "        735.,  743.,  745.,  746.,  751.,  752.,  756.,  765.,  766.,\n",
       "        999.,  217.,  322.,  335.,  347.,  351.,  381.,  412.,  748.,\n",
       "        214.,  275.,  410.,  416.,  420.,  726.,  733.,  317.,  330.,\n",
       "        532.,  505.,  769.,  508.,  749.,  406.,  327.,  721.,  529.,\n",
       "        120.,  180.,  728.,  414.,  521.,  252.,  264.,  739.,  247.,\n",
       "        507.,  770.,  580.,  125.,  333.,  440.,  601.,  760.,  158.,\n",
       "        583.,  763.,  578.,  248.,  764.,  374.,  404.,  393.,  473.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94cit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['XXX', 'ATL', 'WAS', 'NYC', 'TOR', 'BOS', 'HOU', 'MIA', 'CHI',\n",
       "       'LOS', 'CLT', 'DEN', 'DAL', 'DET', 'NEW', 'FTL', 'LVG', 'ORL',\n",
       "       'NOL', 'PIT', 'SFR', 'SPM', 'POO', 'PHI', 'SEA', 'SLC', 'TAM',\n",
       "       'HAM', 'NAS', 'VCV', 'MAA', 'AUS', 'HHW', 'OGG', 'PHO', 'SDP',\n",
       "       'SFB', 'EDA', 'MON', 'CLG', 'DUB', 'FMY', 'YGF', 'SAJ', 'CIN',\n",
       "       'BAL', 'RDU', 'WPB', 'STT', 'OAK', 'NSV', 'SNA', 'OTT', 'X96',\n",
       "       '5KE', 'CLE', 'HAR', 'PSP', 'CHR', 'HAL', 'SAA', 'KOA', 'SHA',\n",
       "       'WIN', 'BGM', 'NCA', 'OPF', 'SAI', 'JFA', 'AGA', 'ONT', 'CLM',\n",
       "       'STL', 'W55', 'CHS', 'SNJ', 'SRQ', 'ANC', 'LNB', 'LIH', 'MIL',\n",
       "       'INP', 'KAN', 'ROC', 'SAC', 'BRO', 'LAR', 'RNO', 'SGR', 'ELP',\n",
       "       'MCA', 'MDT', 'SPE', 'FPR', 'SYR', 'ICT', 'MLB', 'ADS', 'TUC',\n",
       "       'DLR', 'CAE', 'CHA', 'HSV', 'WIL', 'HPN', 'HEF', 'BRG', 'BED',\n",
       "       'DAB', 'JAC', 'FRB', 'SWF', 'KEY', 'PTK', 'MWH', 'X44', 'MYR',\n",
       "       'APF', 'ATW', 'PVD', 'BUF', 'PIE', 'MHT', 'BDL', 'NYL', 'VNY',\n",
       "       '5T6', 'LEX', 'NOR', 'BQN', 'MEM', 'INT', 'CRQ', 'SPO', 'FOK',\n",
       "       'PEV', 'FAR', 'MAF', 'TKI', 'OMA', 'LOU', 'PHF', 'RST', 'MMU',\n",
       "       'CPX', 'SCH', 'RYY', 'PEM', 'JKM', 'LYN', 'OGD', 'NC8', 'MOB',\n",
       "       'SAV', 'HIG', 'CHL', 'WLL', 'MTH', 'AXB', 'SUM', 'ADW', 'SGJ',\n",
       "       'JMZ', 'BLA', 'SSM', 'YIP', 'EPI', 'GSP', 'BHX', 'MND', 'FCA',\n",
       "       'CRP', 'YHC', 'PHU', 'COB', 'OTM', 'STR', 'PSM', 'FWA', 'SYS',\n",
       "       'PEN', 'ABQ', 'HEL', 'DPA', 'CHM', 'EGP', 'POR', 'PIR', 'ORO',\n",
       "       'LUK', 'DER', 'DOU', 'SWE', 'NOO', 'LAN', 'VIC', 'COO', 'NRN',\n",
       "       'REN', 'HTM', 'HID', 'BEL', 'CLS', 'PRO', 'PRE', 'PCF', 'RIF',\n",
       "       'ROS', 'PAR', 'BEE', 'DAC', 'NOG', 'BTN', 'DNS', 'NAC', 'GAL',\n",
       "       'FPT', 'ABG', 'MAS', 'PTL', 'TEC', 'ROO', 'BAU', 'FRI', 'TRO',\n",
       "       'ANA', 'FRE', 'AND', 'LEW', 'NIA', 'PBB', 'THO', 'YSL', 'BEB',\n",
       "       'DLB', 'DNA', 'FTC', 'GPM', 'LAU', 'LCB', 'LLB', 'MOO', 'NEC',\n",
       "       'PHR', 'ROU', 'SKA', 'WHO', 'ANZ', 'BOA', 'CAL', 'MAD', 'MOR',\n",
       "       'ROM', 'WBE', 'AGN', 'CNA', 'SLU', 'FER', 'ALC', 'MET', 'PDN',\n",
       "       'PNH', 'VIB', 'WAL', 'BWA', 'CHT', 'DVL', 'FRT', 'HNN', 'HNS',\n",
       "       'HVR', 'SJO', 'WAR', 'COL', 'TUR', 'ABS', 'BWM', 'CNC', 'RAY',\n",
       "       'VCB', 'MGM', 'MRC', 'PGR', 'LOI', 'ADT', 'NRG', 'CRY', 'ERC',\n",
       "       'FTF', 'FTK', 'SHR', 'MAI', 'NIG', 'NRT', 'VNB', 'FAL', 'LUB',\n",
       "       'RIO', 'LWT'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94port.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,   1.,   2.,   9.,   3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94mode.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94visa.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will retrieve valid city codes and valid port codes to clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(sas_header_file) as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101', '316', '102', '324', '529', '518', '687', '151', '532', '438']\n",
      "[['ALBANIA'], ['ALGERIA'], ['ANDORRA'], ['ANGOLA'], ['ANGUILLA'], ['ANTIGUA-BARBUDA'], ['ARGENTINA '], ['ARMENIA'], ['ARUBA'], ['AUSTRALIA']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regex_matcher = re.compile(r'\\s*(\\d*).*\\'(.*)\\'')\n",
    "\n",
    "valid_cities = OrderedDict()\n",
    "for line in lines[11:298]:\n",
    "    match = regex_matcher.search(line)\n",
    "    abbreviation = match.group(1)\n",
    "    description = match.group(2)\n",
    "    valid_cities[abbreviation]=[description]\n",
    "\n",
    "\n",
    "print(list(valid_cities.keys())[:10])\n",
    "print(list(valid_cities.values())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Example valid cities\n",
    "\n",
    "```\n",
    "{'   101 =  ': ['ALBANIA'], '   316 =  ': ['ALGERIA'], '   102 =  ': ['ANDORRA'], '   324 =  ': ['ANGOLA'], '   529 =  ': ['ANGUILLA'], '   518 =  ': ['ANTIGUA-BARBUDA'], '   687 =  ': ['ARGENTINA '], '   151 =  ': ['ARMENIA'], '   532 =  ': ['ARUBA'], '   438 =  ': ['AUSTRALIA'], '   103 =  ': ['AUSTRIA'], '   152 =  ': ['AZERBAIJAN'], '   512 =  ': ['BAHAMAS'], '   298 =  ': ['BAHRAIN'], '   274 =  ': ['BANGLADESH'], '   513 =  ': ['BARBADOS']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101.0, 316.0, 102.0, 324.0, 529.0, 518.0, 687.0, 151.0, 532.0, 438.0]\n",
      "['ALBANIA', 'ALGERIA', 'ANDORRA', 'ANGOLA', 'ANGUILLA', 'ANTIGUA-BARBUDA', 'ARGENTINA ', 'ARMENIA', 'ARUBA', 'AUSTRALIA']\n"
     ]
    }
   ],
   "source": [
    "# Mapping string to float to fit the dataset\n",
    "adjusted_cities = OrderedDict()\n",
    "for key, value in valid_cities.items():\n",
    "    adjusted_cities[float(key)] = value[0]\n",
    "\n",
    "valid_cities = adjusted_cities\n",
    "print(list(valid_cities.keys())[:10])\n",
    "print(list(valid_cities.values())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "regex_matcher = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_ports = {}\n",
    "for line in lines[303:962]:\n",
    "    match = regex_matcher.search(line)\n",
    "    abbreviation = match.group(1)\n",
    "    description = match.group(2)\n",
    "    valid_ports[abbreviation]=[description]\n",
    "\n",
    "\n",
    "# print(valid_ports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Example valid ports\n",
    "```\n",
    "{'ANC': ['ANCHORAGE, AK         '], 'BAR': ['BAKER AAF - BAKER ISLAND, AK'], 'DAC': ['DALTONS CACHE, AK     '], 'PIZ': ['DEW STATION PT LAY DEW, AK'], 'DTH': ['DUTCH HARBOR, AK      '], 'EGL': ['EAGLE, AK             '], 'FRB': ['FAIRBANKS, AK         '], 'HOM': ['HOMER, AK             '], 'HYD': ['HYDER, AK             '], 'JUN': ['JUNEAU, AK            '],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We shall clean the cities and ports to ensure that only valid codes are present. Also, we will remove i94mode that are not a number eg 'NaN' values. We will only keep relevant columns that will be used in the fact or dimension tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_spark_df(file):\n",
    "    return spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "def clean_data(df):\n",
    "    target_columns = ['i94yr', 'i94mon', 'i94cit', 'i94port', 'i94mode', 'i94bir', 'arrdate', 'i94visa']\n",
    "    \n",
    "    df = df.filter(df.i94cit.isin(valid_cities))\n",
    "    df = df.filter(df.i94port.isin(list(valid_ports.keys())))\n",
    "    df = df.filter(df.i94mode != 'NaN')\n",
    "\n",
    "    return df.select(*target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+------+-------+-------+\n",
      "| i94yr|i94mon|i94cit|i94port|i94mode|i94bir|arrdate|i94visa|\n",
      "+------+------+------+-------+-------+------+-------+-------+\n",
      "|2016.0|   4.0| 101.0|    WAS|    1.0|  55.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  28.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|   4.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  57.0|20545.0|    1.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  63.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  57.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  46.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  48.0|20545.0|    1.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  52.0|20545.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    TOR|    1.0|  33.0|20545.0|    2.0|\n",
      "+------+------+------+-------+-------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df = clean_data(load_spark_df(fname))\n",
    "immigration_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def map_city(city):\n",
    "    for key in valid_port:\n",
    "        if city.lower() in valid_port[key][0].lower():\n",
    "            return key\n",
    "\n",
    "def map_df(df):\n",
    "    df\n",
    "    \n",
    "immigration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_csv(temperature_fname, sep=',')\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-42.70399999999999\n",
      "39.650999999999996\n",
      "Check for non numeric AverageTemperature\n",
      "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
      "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
      "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
      "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
      "\n",
      "   Country Latitude Longitude  \n",
      "1  Denmark   57.05N    10.33E  \n",
      "2  Denmark   57.05N    10.33E  \n",
      "3  Denmark   57.05N    10.33E  \n",
      "Check for non numeric AverageTemperatureUncertainty\n",
      "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
      "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
      "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
      "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
      "\n",
      "   Country Latitude Longitude  \n",
      "1  Denmark   57.05N    10.33E  \n",
      "2  Denmark   57.05N    10.33E  \n",
      "3  Denmark   57.05N    10.33E  \n",
      "Check for non numeric Latitude\n",
      "Empty DataFrame\n",
      "Columns: [dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]\n",
      "Index: []\n",
      "Check for non numeric Longitude\n",
      "Empty DataFrame\n",
      "Columns: [dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(min(temp_df.AverageTemperature))\n",
    "print(max(temp_df.AverageTemperature))\n",
    "\n",
    "print(\"Check for non numeric AverageTemperature\")\n",
    "print(temp_df[pd.isnull(temp_df.AverageTemperature)][:3])\n",
    "\n",
    "print(\"Check for non numeric AverageTemperatureUncertainty\")\n",
    "print(temp_df[pd.isnull(temp_df.AverageTemperatureUncertainty)][:3])\n",
    "\n",
    "print(\"Check for non numeric Latitude\")\n",
    "print(temp_df[temp_df.Latitude.apply(lambda x: x.isnumeric())])\n",
    "\n",
    "print(\"Check for non numeric Longitude\")\n",
    "print(temp_df[temp_df.Longitude.apply(lambda x: x.isnumeric())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will be cleaning null values and only selecting the relevant columns from temperature dataset for the dimension and fact tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def csv_to_spark_df(file):\n",
    "    return spark.read.format(\"csv\").option(\"header\", \"true\").load(file)\n",
    "\n",
    "def clean_temp_data(df):\n",
    "    target_columns = ['dt', 'AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude']\n",
    "    df = df.filter(temp_df.AverageTemperature != 'NaN')\n",
    "\n",
    "    return df.select(*target_columns)\n",
    "\n",
    "temp_df = clean_temp_data(csv_to_spark_df(temperature_fname))\n",
    "temp_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def map_country(city):\n",
    "    for key, value in valid_cities.items():\n",
    "        if city.lower() == value.lower():\n",
    "            return key\n",
    "\n",
    "def map_temp_data(df):\n",
    "    df = df.withColumn(\"CityCode\", map_country(temp_df.Country))\n",
    "    return df.filter(df.CityCode != 'null')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-------+--------+---------+--------+--------+\n",
      "|        dt|AverageTemperature| City|Country|Latitude|Longitude|PortCode|CityCode|\n",
      "+----------+------------------+-----+-------+--------+---------+--------+--------+\n",
      "|1743-11-01|             6.068|Ã…rhus|Denmark|  57.05N|   10.33E|   108.0|   108.0|\n",
      "|1744-04-01|5.7879999999999985|Ã…rhus|Denmark|  57.05N|   10.33E|   108.0|   108.0|\n",
      "|1744-05-01|            10.644|Ã…rhus|Denmark|  57.05N|   10.33E|   108.0|   108.0|\n",
      "|1744-06-01|14.050999999999998|Ã…rhus|Denmark|  57.05N|   10.33E|   108.0|   108.0|\n",
      "|1744-07-01|            16.082|Ã…rhus|Denmark|  57.05N|   10.33E|   108.0|   108.0|\n",
      "+----------+------------------+-----+-------+--------+---------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df = map_temp_data(temp_df)\n",
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Dimension Tables\n",
    "\n",
    "Immigration will contain the following data from the I94 dataset.\n",
    "```\n",
    "I94YR   - 4 digit year\n",
    "I94MON  - Numeric month\n",
    "I94CIT  - Country visited\n",
    "I94PORT - Port visited\n",
    "I94MODE - Mode of travel\n",
    "I94BIR  - Age of Respondent in years\n",
    "ARRDATE - Arrival date\n",
    "I94VISA - Purpose of visit (Business/Pleasure/Student)\n",
    "```\n",
    "\n",
    "Temperature will contain the following columns from the temperature dataset.\n",
    "\n",
    "```\n",
    "dt                 - Date of recording\n",
    "CityCode           - Code representing a city\n",
    "AverageTemperature - Average temperature for the day\n",
    "City               - City name\n",
    "Country            - Country name\n",
    "Latitude           - Latitude\n",
    "Longitude          - Longitude\n",
    "```\n",
    "\n",
    "Fact Table:\n",
    "```\n",
    "I94YR   - 4 digit year\n",
    "I94MON  - Numeric month\n",
    "I94CIT  - Country visited\n",
    "I94PORT - Port visited\n",
    "I94MODE - Mode of travel\n",
    "I94BIR  - Age of Respondent in years\n",
    "ARRDATE - Arrival date\n",
    "I94VISA - Purpose of visit (Business/Pleasure/Student)\n",
    "AverageTemperature - Average temperature for the day\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    " 1. Data cleaning\n",
    " 2. Transform collumns to standardised code (eg country code)\n",
    " 3. Transform to dimension tables\n",
    " 4. Create fact table by joining on the city code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"i94port\") \\\n",
    "    .parquet(\"/tables/immigration.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"CityCode\") \\\n",
    "    .parquet(\"/tables/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temporary views \n",
    "immigration_df.createOrReplaceTempView(\"immigration_view\")\n",
    "temp_df.createOrReplaceTempView(\"temperature_view\")\n",
    "\n",
    "# Create the fact table by joining on city code\n",
    "fact_table = spark.sql(\"\"\"\n",
    "SELECT immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as port,\n",
    "       immigration_view.i94mode as travel_mode,\n",
    "       immigration_view.i94bir as birthday,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON (immigration_view.i94cit = temperature_view.CityCode)\n",
    "\"\"\")\n",
    "\n",
    "# Write fact table \n",
    "fact_table.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"city\") \\\n",
    "    .parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_count(df):\n",
    "    return df.count() > 0\n",
    "\n",
    "print(check_count(fact_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Integrity Check\n",
    "imm_city_count = df_immigration.select(col(\"i94cit\")).distinct().count()\n",
    "temp_city_count = temp_df.select(col(\"CityCode\")).distinct().count()\n",
    "fact_city_count = fact_table.select(col(\"city\")).distinct().count()\n",
    "\n",
    "print(temp_city_count <= imm_city_count)\n",
    "print(fact_city_count <= imm_city_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "\n",
    "Dimension Tables\n",
    "\n",
    "Immigration will contain the following data from the I94 dataset.\n",
    "```\n",
    "I94YR   - 4 digit year\n",
    "I94MON  - Numeric month\n",
    "I94CIT  - Country visited\n",
    "I94PORT - Port visited\n",
    "I94MODE - Mode of travel\n",
    "I94BIR  - Age of Respondent in years\n",
    "ARRDATE - Arrival date\n",
    "I94VISA - Purpose of visit (Business/Pleasure/Student)\n",
    "```\n",
    "\n",
    "Temperature will contain the following columns from the temperature dataset.\n",
    "\n",
    "```\n",
    "dt                 - Date of recording\n",
    "CityCode           - Code representing a city\n",
    "AverageTemperature - Average temperature for the day\n",
    "City               - City name\n",
    "Country            - Country name\n",
    "Latitude           - Latitude\n",
    "Longitude          - Longitude\n",
    "```\n",
    "\n",
    "Fact Table:\n",
    "```\n",
    "I94YR   - 4 digit year\n",
    "I94MON  - Numeric month\n",
    "I94CIT  - Country visited\n",
    "I94PORT - Port visited\n",
    "I94MODE - Mode of travel\n",
    "I94BIR  - Age of Respondent in years\n",
    "ARRDATE - Arrival date\n",
    "I94VISA - Purpose of visit (Business/Pleasure/Student)\n",
    "AverageTemperature - Average temperature for the day\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "First, we use Pandas to perform data exploration to get a better sense of the data and identify potential issues to fix. We use Apache Spark for the overall data pipeline to support the processing of large datasets. Also we use the parquet format that can be saved into S3 for efficient data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "The data should be updated daily as the dimension tables captures daily information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " We could manually resize the number of core nodes in the running Spark cluster to a suitable size that meets the business requirements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "    We could set up a schedule job using Apache Airflow to update the database at the specified timing. Then, push a notification to the dashboard to retrieve the data from our database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "     We could provision database read replicas if there are multiple users accessing directly to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
